data$GarageType <- as.factor(data$GarageType1)
# chainging the NA's of the FireplaceQu1 to None
data$LotFrontage1 <- as.character(data$LotFrontage)
data$LotFrontage1[which(is.na(data$LotFrontage))] <- "None"
data$LotFrontage <- as.factor(data$LotFrontage1)
# chainging the NA's of the GarageArea to None
data$GarageArea1 <- as.character(data$GarageArea)
data$GarageArea1[which(is.na(data$GarageArea))] <- "None"
data$GarageArea1[which((data$GarageArea == "None"))] <- "0"
data$GarageArea <- as.factor(data$GarageArea1)
# chainging the NA's of the GarageQual to None
data$GarageQual1 <- as.character(data$GarageQual)
data$GarageQual1[which(is.na(data$GarageQual))] <- "None"
data$GarageQual <- as.factor(data$GarageQual1)
data$GarageQual <- data$GarageQual1
featureEng <- function(data){
###Combining existing features###
data[["GarageScore"]] <- data[["GarageArea"]] * data[["GarageQual"]]
data[["TotalSF"]] <- data[["TotalBsmtSF"]] + data[["X1stFlrSF"]] + data[["X2ndFlrSF"]]
data[["TotalBath"]] <- data[["BsmtFullBath"]] + (0.5 * data[["BsmtHalfBath"]]) + data[["FullBath"]] + (0.5 * data[["HalfBath"]])
return(data)
}
data <- featureEng(data)
# many of the varibles that we have should be numeric but those are given as catofirical factors
# let's change them to numeric first
data[["ExterQual"]] <- as.numeric(factor(data[["ExterQual"]], levels=c("None","Po","Fa", "TA", "Gd", "Ex")))
data[["ExterCond"]] <- as.numeric(factor(data[["ExterCond"]], levels=c("None","Po","Fa", "TA", "Gd", "Ex")))
data[["BsmtQual"]] <- as.numeric(factor(data[["BsmtQual"]], levels=c("None","Po", "Fa", "TA", "Gd", "Ex")))
data[["BsmtCond"]] <- as.numeric(factor(data[["BsmtCond"]], levels=c("None","Po", "Fa", "TA", "Gd", "Ex")))
data[["BsmtExposure"]] <- as.numeric(factor(data[["BsmtExposure"]], levels=c("None","No", "Mn", "Av", "Gd")))
data[["BsmtFinType1"]] <- as.numeric(factor(data[["BsmtFinType1"]], levels=c("None","Unf","LwQ","Rec","BLQ","ALQ","GLQ")))
data[["BsmtFinType2"]] <- as.numeric(factor(data[["BsmtFinType2"]], levels=c("None","Unf","LwQ","Rec","BLQ","ALQ","GLQ")))
data[["HeatingQC"]] <- as.numeric(factor(data[["HeatingQC"]], levels=c("None","Po", "Fa", "TA", "Gd", "Ex")))
data[["KitchenQual"]] <- as.numeric(factor(data[["KitchenQual"]], levels=c("None","Po", "Fa", "TA", "Gd", "Ex")))
data[["GarageQual"]] <- as.numeric(factor(data[["GarageQual"]], levels=c("None","Po", "Fa", "TA", "Gd", "Ex")))
data[["GarageCond"]] <- as.numeric(factor(data[["GarageCond"]], levels=c("None","Po", "Fa", "TA", "Gd", "Ex")))
str(data$GarageQual)
str(data$GarageArea)
data$GarageArea <- as.numeric(data$GarageArea)
featureEng <- function(data){
###Combining existing features###
data[["GarageScore"]] <- data[["GarageArea"]] * data[["GarageQual"]]
data[["TotalSF"]] <- data[["TotalBsmtSF"]] + data[["X1stFlrSF"]] + data[["X2ndFlrSF"]]
data[["TotalBath"]] <- data[["BsmtFullBath"]] + (0.5 * data[["BsmtHalfBath"]]) + data[["FullBath"]] + (0.5 * data[["HalfBath"]])
return(data)
}
data <- featureEng(data)
data <- subset(data, select = -Electrical1)
data <- subset(data, select = -MasVnrArea1)
data <- subset(data, select = -MasVnrType1)
data <- subset(data, select = -BsmtFinType21)
data <- subset(data, select = -BsmtCond1)
data <- subset(data, select = -BsmtQual1)
data <- subset(data, select = -BsmtExposure1)
data <- subset(data, select = -GarageFinish1)
data <- subset(data, select = -GarageQual1)
data <- subset(data, select = -GarageCond1)
data <- subset(data, select = -GarageYrBlt1)
data <- subset(data, select = -GarageType1)
data <- subset(data, select = -LotFrontage1)
data <- subset(data, select = -FireplaceQu1)
data <- subset(data, select = -BsmtFinType11)
data <- subset(data, select = -GarageArea1)
data <- subset(data, select = -GarageQual1)
data <- subset(data, select = -ExterQual1)
data <- subset(data, select = -ExterQual)
colnames(data)
# checking if the work is done
Na <- sapply(data, function(x) sum(is.na(x)))
Na.sum <- data.frame(index = names(data), Mis <- Na)
Na.sum[Na.sum$Mis > 0,]
# Now that we have cleaned the data we can start our analysis and feature selection
#  we will build a rough linearmodel for every variable
# to check which ones are important and if these variables are independet or dependent
par(mfrow=c(2,2))
train1 <- subset(data[1:1460,])
str(train1)
data$GarageYrBlt <- as.numeric_version(data$GarageYrBlt)
head(data$GarageYrBlt)
which.max(data$GarageYrBlt)
which.min(data$GarageYrBlt)
data$GarageYrBlt <- as.numeric(factor(data$GarageYrBlt))
head(data$GarageYrBlt)
tail(data$GarageYrBlt)
# Now that we have cleaned the data we can start our analysis and feature selection
#  we will build a rough linearmodel for every variable
# to check which ones are important and if these variables are independet or dependent
par(mfrow=c(2,2))
train1 <- subset(data[1:1460,])
for (i in 2:(ncol(train1)-1)) {
if (is.numeric(train1[,i]) == "TRUE"){
plot(train1$SalePrice ~ train[,i],main=names(train1)[i], xlab=names(train1)[i],col="blue")
reg_line <- lm(train1$SalePrice ~ train1[,i])
abline(reg_line,col="red")
}
}
library(corrplot)
par(mfrow=c(1,1))
train1$SalePrice <- as.numeric(train$SalePrice)
corrplot(cor(train1[2:ncol(train1)] %>% select_if(is.numeric)),
type = "upper", method = "number", tl.cex = 0.7, tl.col="black",number.cex = 0.5)
corrplot(cor(train1[2:ncol(train1)] %>% select_if(is.numeric)),
type = "full", method = "number", tl.cex = 0.7, tl.col="black",number.cex = 0.5)
library(doSNOW)
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)
train_index <- sample(1:nrow(train1), 0.8 * nrow(train1))
test_index <- setdiff(1:nrow(train1), train_index)
# Build X_train, X_test
X_train <- train1[train_index, ]
colnames(train1)
X_test <- train1[test_index,]
t <- train1[test_index, c(77)]
library(randomForest)
set.seed(121)
rf.2 <- randomForest(SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + BsmtFinSF1 +
TotalBsmtSF + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + Fireplaces +
GarageCars + + BsmtUnfSF + GarageArea +
WoodDeckSF + Neighborhood, data = X_train, importance = T, ntree = 3000, mtry = 7)
# exploring the model
rf.2
plot(rf.2)
varImpPlot(rf.2)
pp <- predict(rf.2, X_test, response = T)
str(pp)
str(t)
pp <- as.numeric(pp)
t <- as.numeric(t)
pp_2 <- predict(rf.2, test, response = T)
submit_rf_1 <- data.frame(Id = test$Id, saleprice = pp)
submit_rf_1 <- data.frame(Id = test$Id, saleprice = pp_2)
write.csv(submit_rf_1, file = "rf_sub_2019_11_20.csv", row.names = FALSE)
set.seed(11)
rf.3 <- randomForest(SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + BsmtFinSF1 +
TotalBsmtSF + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + Fireplaces +
GarageCars + + BsmtUnfSF + GarageArea +
WoodDeckSF + Neighborhood + TotalBath +
GarageScore + TotalSF, data = X_train, importance = T, ntree = 3000, mtry = 7)
rf.3
# plotting the model shows us where our error rate stabalizes
plot(rf.3)
# here our model stabalizes error rate around 1750 trees
varImpPlot(rf.3)
# checking the number of trees with lowest mean square error
which.min(rf.3$mse)
pp2 <- predict(rf.3, X_test, response = T)
pp2
head(pp2)
head(t)
otest <- subset(data[1461:2919,])
pp3 <- predict(rf.3, otest, response = T)
submit_rf_2 <- data.frame(Id = test$Id, saleprice = pp3)
write.csv(submit_rf_2, file = "rf_sub_2019_11_20.1.csv", row.names = FALSE)
library(xgboost)
library(Matrix)
trainm <- sparse.model.matrix(SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + BsmtFinSF1 +
TotalBsmtSF + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + Fireplaces +
GarageCars + + BsmtUnfSF + GarageArea +
WoodDeckSF + Neighborhood + TotalBath +
GarageScore + TotalSF, data = X_train)
train_label <- X_train[,"SalePrice"]
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)
testm <- sparse.model.matrix(SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + BsmtFinSF1 +
TotalBsmtSF + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + Fireplaces +
GarageCars + + BsmtUnfSF + GarageArea +
WoodDeckSF + Neighborhood + TotalBath +
GarageScore + TotalSF, data = X_test)
test_label <- X_test[,"SalePrice"]
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
#parameters
str(train_label)
nc <- length(unique(train_label))
nc
param <- list(colsample_bytree = 1,
subsample = .6,
booster = "gbtree",
max_depth = 8,
eta = 0.05,
min_child_weight = 2,
num_class = nc,
eval_metric = "rmse",
objective="reg:linear",
gamma = 0.01)
# making model
bstSparse <-
xgb.train(params = param,
data = train_matrix,
nrounds = 400,
watchlist = watchlist,
verbose = TRUE,
print_every_n = 50,
nthread = 2)
watchlist <- list(train = train_matrix, test = test_matrix)
# making model
bstSparse <-
xgb.train(params = param,
data = train_matrix,
nrounds = 400,
watchlist = watchlist,
verbose = TRUE,
print_every_n = 50,
nthread = 2)
trainm <- sparse.model.matrix(SalePrice ~. -77, data = X_train)
trainm <- sparse.model.matrix(SalePrice ~. -c(77), data = X_train)
trainm <- sparse.model.matrix(SalePrice ~. -c("SalePrice"), data = X_train)
trainm <- sparse.model.matrix(SalePrice ~. , data = X_train[,-77])
train_label <- X_train[,"SalePrice"]
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)
library(xgboost)
library(Matrix)
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)
testm <- sparse.model.matrix(SalePrice ~. , data = X_test)
test_label <- X_test[,"SalePrice"]
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
str(train_label)
nc <- length(unique(train_label))
nc
param <- list(colsample_bytree = 1,
subsample = .6,
booster = "gbtree",
max_depth = 8,
eta = 0.05,
min_child_weight = 2,
num_class = nc,
eval_metric = "rmse",
objective="reg:linear",
gamma = 0.01)
watchlist <- list(train = train_matrix, test = test_matrix)
library(doSNOW)
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)
# making model
bstSparse <-
xgb.train(params = param,
data = train_matrix,
nrounds = 400,
watchlist = watchlist,
verbose = TRUE,
print_every_n = 50,
nthread = 2)
train_matrix
train_label <- X_train[,"OverallQual" + "YearBuilt" + "YearRemodAdd" + "BsmtFinSF1" +
"TotalBsmtSF" + "X1stFlrSF" + "GrLivArea" + "FullBath" + "TotRmsAbvGrd" +"Fireplaces" +
"GarageCars"  + "BsmtUnfSF" + "GarageArea" +
"WoodDeckSF" + "Neighborhood" + "TotalBath" +
"GarageScore" + "TotalSF"]
train_label <- X_train["SalePrice","OverallQual" + "YearBuilt" + "YearRemodAdd" + "BsmtFinSF1" +
"TotalBsmtSF" + "X1stFlrSF" + "GrLivArea" + "FullBath" + "TotRmsAbvGrd" +"Fireplaces" +
"GarageCars"  + "BsmtUnfSF" + "GarageArea" +
"WoodDeckSF" + "Neighborhood" + "TotalBath" +
"GarageScore" + "TotalSF"]
train_label <- X_train["SalePrice"+"OverallQual" + "YearBuilt" + "YearRemodAdd" + "BsmtFinSF1" +
"TotalBsmtSF" + "X1stFlrSF" + "GrLivArea" + "FullBath" + "TotRmsAbvGrd" +"Fireplaces" +
"GarageCars"  + "BsmtUnfSF" + "GarageArea" +
"WoodDeckSF" + "Neighborhood" + "TotalBath" +
"GarageScore" + "TotalSF"]
trainm <- sparse.model.matrix(SalePrice ~. , data = X_train)
train_label <- X_train[,"SalePrice"]
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)
testm <- sparse.model.matrix(SalePrice ~. , data = X_test)
test_label <- X_test[,"SalePrice"]
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
#parameters
str(train_label)
nc <- length(unique(train_label))
nc
param <- list(colsample_bytree = 1,
subsample = .6,
booster = "gbtree",
max_depth = 8,
eta = 0.05,
min_child_weight = 2,
num_class = nc,
eval_metric = "rmse",
objective="reg:linear",
gamma = 0.01)
watchlist <- list(train = train_matrix, test = test_matrix)
# making model
bstSparse <-
xgb.train(params = param,
data = train_matrix,
nrounds = 400,
watchlist = watchlist,
verbose = TRUE,
print_every_n = 50,
nthread = 2)
train_label <- X_train[,]
test_label <- X_test[,]
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
trainm <- sparse.model.matrix(SalePrice ~. , data = X_train)
train_label <- X_train[,"SalePrice"]
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)
testm <- sparse.model.matrix(SalePrice ~. , data = X_test)
test_label <- X_test[,"SalePrice"]
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
#parameters
str(train_label)
nc <- length(unique(train_label))
nc
param <- list(colsample_bytree = 1,
subsample = .6,
booster = "gbtree",
max_depth = 8,
eta = 0.05,
min_child_weight = 2,
num_class = nc,
eval_metric = "rmse",
objective="reg:linear",
gamma = 0.01)
watchlist <- list(train = train_matrix, test = test_matrix)
# making model
bstSparse <-
xgboost(params = param,
data = train_matrix,
nrounds = 400,
watchlist = watchlist,
verbose = TRUE,
print_every_n = 50,
nthread = 2)
library(xgboost)
library(Matrix)
trainm <- sparse.model.matrix(SalePrice ~. , data = X_train)
train_label <- X_train[,"SalePrice"]
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)
testm <- sparse.model.matrix(SalePrice ~. , data = X_test)
test_label <- X_test[,"SalePrice"]
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
param <- list(colsample_bytree = 1,
subsample = .6,
booster = "gbtree",
max_depth = 8,
eta = 0.05,
min_child_weight = 2,
num_class = nc,
eval_metric = "rmse",
objective="reg:linear",
gamma = 0.01)
watchlist <- list(train = train_matrix, test = test_matrix)
sp <- xgboost(data = train_matrix, label = train_label, params = param)
sp <- xgboost(data = train_matrix, label = train_label,nrounds = 200, params = param)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 100,
lambda = 0.01,
lambda_bias = 0.01,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 200,
lambda = 0.01,
lambda_bias = 0.01,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 3000,
lambda = 0.01,
lambda_bias = 0.01,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 3000,
lambda = 0.01,
lambda_bias = 0.01,
eta = 0.5,
alpha = 0)
plot(x)
plot(x)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.01,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.03,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.1,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
xg <- predict(x, data = test_matrix)
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)
xg <- predict(x, data = test_matrix)
xg <- predict(x, test_matrix)
xg
head(xg)
xg <- predict(x, test)
xg <- predict(x, X_test)
xx <- sparse.model.matrix(SalePrice ~., data = otest)
xg <- predict(x, xx)
xx
xx
xx
submit_xg_3 <- data.frame(Id = test$Id, saleprice = xx)
submit_xg_3 <- (Id = test$Id, saleprice = xx)
submit_xg_3 <- as.matrix(Id = test$Id, saleprice = xx)
submit_xg_3 <- as.matrix( test$Id, xx)
write.csv(submit_rf_2, file = "xg_sub_2019_11_20.0.1.csv", row.names = FALSE)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.1,
lambda_bias = 0.01,
eta = 0.6,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.1,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.15,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0.05,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 300,
lambda = 0,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 3000,
lambda = 0,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 1000,
lambda = 0,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 500,
lambda = 0,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
xg <- predict(x, test_matrix)
xg
head(t)
x <-xgboost(data = train_matrix,
booster = "gbtree",
objective = "reg:linear",
max.depth = 5,
nrounds = 400,
lambda = 0,
lambda_bias = 0.01,
eta = 1.0,
alpha = 0)
xg <- predict(x, test_matrix)
xg
head(xg)
head(t)
xg <- predict(x, xx)
xx
write.csv(submit_xg_3, file = "xg_sub_2019_11_20.0.2.csv", row.names = FALSE)
